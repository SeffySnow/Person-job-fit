# -*- coding: utf-8 -*-
"""Direct Study

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dflAmFcTNMb4FZW3yD-eKIqiydbj4Otr

https://www.kaggle.com/datasets/ayushtankha/70k-job-applicants-data-human-resource
"""

!pip install kaggle
!kaggle datasets download -d ayushtankha/70k-job-applicants-data-human-resource

import zipfile

with zipfile.ZipFile('70k-job-applicants-data-human-resource.zip', 'r') as zip_ref:
    zip_ref.extractall('70k-job-applicants-data-human-resource')

import pandas as pd

df = pd.read_csv('/content/70k-job-applicants-data-human-resource/stackoverflow_full.csv', index_col=0)

df.info()

# Data Cleaning

# Drop the 'Unnamed: 0' column as it is an index


# Handle missing values
df['HaveWorkedWith'].fillna('Unknown', inplace=True)

# Replace 'Yes' with 1 and 'No' with 0
df_cleaned = df.replace({'Yes': 1, 'No': 0})

# Feature Selection
# We will use 'Age', 'EdLevel', 'Gender', 'YearsCode', 'YearsCodePro', 'Country', 'PreviousSalary', and 'ComputerSkills' for clustering

# # Encoding categorical variables
# df_encoded = pd.get_dummies(df_cleaned, columns=['Age', 'EdLevel', 'Gender', 'Country'], drop_first=True)

# Normalize the data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
# df_normalized = df_encoded.copy()
# df_cleaned[['YearsCode', 'YearsCodePro', 'PreviousSalary', 'ComputerSkills']] = scaler.fit_transform(df_cleaned[['YearsCode', 'YearsCodePro', 'PreviousSalary', 'ComputerSkills']])

# # Display the first few rows of the cleaned, encoded, and normalized dataset
# df_cleaned.head()

"""#visualiztion"""

# import matplotlib.pyplot as plt
# import seaborn as sns

# # Set up the visual style
# sns.set(style="whitegrid")

# # Figure size
# plt.figure(figsize=(15, 20))

# # Subplot for Age distribution
# plt.subplot(3, 2, 1)
# sns.countplot(data=df, x='Age', order=df['Age'].value_counts().index)
# plt.title('Age Distribution')

# # Subplot for Accessibility
# plt.subplot(3, 2, 2)
# sns.countplot(data=df, x='Accessibility', order=df['Accessibility'].value_counts().index)
# plt.title('Accessibility Status')

# # Subplot for Education Level (EdLevel)
# plt.subplot(3, 2, 3)
# sns.countplot(data=df, x='EdLevel', order=df['EdLevel'].value_counts().index)
# plt.xticks(rotation=45, ha='right')
# plt.title('Education Level Distribution')

# # Subplot for Gender distribution
# plt.subplot(3, 2, 4)
# sns.countplot(data=df, x='Gender', order=df['Gender'].value_counts().index)
# plt.title('Gender Distribution')

# # Subplot for MainBranch
# plt.subplot(3, 2, 5)
# sns.countplot(data=df, x='MainBranch', order=df['MainBranch'].value_counts().index)
# plt.title('Main Professional Branch')

# # Subplot for HaveWorkedWith (Top 10)
# plt.subplot(3, 2, 6)
# # Split 'HaveWorkedWith' and count occurrences of each technology
# tech_series = df['HaveWorkedWith'].str.split(';', expand=True).stack()
# tech_count = tech_series.value_counts().head(10)
# sns.barplot(x=tech_count.index, y=tech_count.values)
# plt.xticks(rotation=45, ha='right')
# plt.title('Top 10 Technologies Worked With')

# plt.figure(figsize=(15, 6))
# country_count = df['Country'].value_counts().head(10)  # Show top 10 countries
# sns.barplot(x=country_count.index, y=country_count.values)
# plt.xticks(rotation=45, ha='right')
# plt.title('Top 10 Countries')
# plt.xlabel('Country')
# plt.ylabel('Number of Candidates')
# plt.show()

# # Adjust layout
# plt.tight_layout()
# plt.show()

"""Declaring a job position and filter the dataset:

*   age < 35
*   Dev - front end developer
*   Skills: JavaScript, Docker, HTML/CSS, TypeScript
* location: USA

Second Job Position:

*   age < 35
*   Dev - Developer - PM
*   Skills: SQL , AWS, Docker
* location : Ca
* Education : Master Or PHD

third Job Position:
*   age > 35
*   Dev - Back end developer
*   Skills: SQL , AWS, Docker , Python
* Location: India

Forth Job Position:

*   age < 35
*   Dev - front end developer
*   Skills: JavaScript, Docker, HTML/CSS, TypeScript
* Location:  Brazil

Fifth Job Position:
*   age > 35
*   NotDev

# Filtering data
"""

# Define the criteria for the job position
age_criteria = df['Age'] == '<35'
main_branch_criteria = df['MainBranch'] == 'Dev'
country_criteria = df['Country'] == 'United States of America'
skills_criteria = df['HaveWorkedWith'].apply(lambda x: all(skill in str(x) for skill in ['JavaScript', 'Docker', 'HTML/CSS', 'TypeScript']))

# Filter the data based on the criteria
filtered_df = df[age_criteria & main_branch_criteria & country_criteria & skills_criteria]

# Display the filtered data
# filtered_df.head(),
filtered_df=filtered_df.drop(["Age", "MainBranch","Country"], axis=1)

filtered_df

"""# Prepare Data"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import pandas as pd
import numpy as np
cat = [ 'EdLevel', 'Gender', 'MentalHealth','Accessibility', 'Employment']
num = ['YearsCode', 'YearsCodePro', 'PreviousSalary', 'ComputerSkills']
y= filtered_df['Employed']
X = filtered_df.drop(['Employed'], axis=1)
onehot = OneHotEncoder()
scaler = MinMaxScaler()
tranformer = ColumnTransformer([
    ('onehot', onehot, cat),
    ('scaler', scaler, num)
], remainder = 'passthrough')
data = tranformer.fit_transform(X)
cat_columns = tranformer.named_transformers_['onehot'].get_feature_names_out(cat)
num_columns = num  # Since scaler doesn't change column names
new_column_names = list(cat_columns) + list(num_columns) + ['Skills']
data.shape
# Convert the transformed data back to a DataFrame with new column names
transformed_df = pd.DataFrame(data, columns=new_column_names)
l=[]
for person in transformed_df['Skills']:

  all = person.split(";")
  for skill in all:

    if skill not in l:
      l.append(skill)



skills_set= set(l)
for skill in skills_set:
    transformed_df[f'Skill_{skill}'] = transformed_df['Skills'].apply(lambda x: 1 if skill in x else 0)

transformed_df.drop(['Skills'], axis=1, inplace=True)

"""#Model"""

from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, IsolationForest
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score
from scipy.spatial.distance import pdist, squareform

X_train , X_test, y_train, y_test = train_test_split(transformed_df, y, test_size=0.2, random_state=42)

rf_model = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=42)


dt_model = DecisionTreeClassifier(max_depth=12, min_samples_split=10)


svm_model = SVC(kernel='poly', degree=6, probability=True, random_state=42)


voting_model = VotingClassifier(estimators=[
    ('rf', rf_model),
    ('dt', dt_model),
    ('svm', svm_model)
], voting='soft')


voting_model.fit(X_train, y_train)
voting_pred = voting_model.predict(X_test)


voting_precision = precision_score(y_test, voting_pred)
voting_recall = recall_score(y_test, voting_pred)
voting_f1 = f1_score(y_test, voting_pred)

print(f'Voting Classifier Precision: {voting_precision:.4f}')
print(f'Voting Classifier Recall: {voting_recall:.4f}')
print(f'Voting Classifier F1 Score: {voting_f1:.4f}')
# (voting_pred==1)

"""#Choosing Potential Candidates"""

# import torch
y_test = y_test.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
# if isinstance(y_test, torch.Tensor):
#     y_test = y_test.numpy()
# selected_indices = np.where((voting_pred== 1))
# selected_nodes = X_test.iloc[selected_indices]

def find_selected_nodes(X):

    predictions = voting_model.predict(X)
    selected_indices = np.where((predictions== 1))

    selected_nodes = X.iloc[selected_indices]
    return selected_nodes

selected_nodes = find_selected_nodes(X_test)



"""#Outlier detection"""

import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import pdist, squareform
from sklearn.preprocessing import MinMaxScaler

def find_outlier_in_set(selected_nodes, contamination_level=0.1):
    """
    Find the most outstanding outlier in the given set of selected_nodes.

    Args:
        selected_nodes (DataFrame): A DataFrame containing the selected nodes.
        contamination_level (float): The contamination level for the Isolation Forest model.

    Returns:
        tuple: The index of the most outstanding outlier and the outlier itself (as a Series).
    """
    # Convert selected nodes to a NumPy array
    selected_nodes_array = selected_nodes.select_dtypes(include=np.number).to_numpy()

    # Calculate the Euclidean distance for selected nodes
    euclidean_distances = squareform(pdist(selected_nodes_array, metric='euclidean'))

    # Initialize the Isolation Forest model for Euclidean distances
    iso_forest_euclidean = IsolationForest(contamination=contamination_level, random_state=42)

    # Fit the model on the Euclidean distances
    iso_forest_euclidean.fit(euclidean_distances)

    # Predict outlier scores for Euclidean distances
    outlier_scores_euclidean = iso_forest_euclidean.decision_function(euclidean_distances)

    # Calculate the cosine similarity for selected nodes
    cosine_similarities = cosine_similarity(selected_nodes)
    # Convert cosine similarity to cosine distance (1 - similarity)
    cosine_distances = 1 - cosine_similarities

    # Initialize the Isolation Forest model for cosine distances
    iso_forest_cosine = IsolationForest(contamination=contamination_level, random_state=42)

    # Fit the model on the cosine distances
    iso_forest_cosine.fit(cosine_distances)

    # Predict outlier scores for cosine distances
    outlier_scores_cosine = iso_forest_cosine.decision_function(cosine_distances)

    # Normalize the outlier scores
    scaler = MinMaxScaler()
    normalized_scores_euclidean = scaler.fit_transform(outlier_scores_euclidean.reshape(-1, 1)).flatten()
    normalized_scores_cosine = scaler.fit_transform(outlier_scores_cosine.reshape(-1, 1)).flatten()

    # Combine the normalized outlier scores
    combined_scores = normalized_scores_euclidean + normalized_scores_cosine

    # Identify the most outstanding outlier
    most_outstanding_outlier_index = np.argmin(combined_scores)
    most_outstanding_outlier_df_index = selected_nodes.index[most_outstanding_outlier_index]

    # Return the index and the outlier itself
    return most_outstanding_outlier_df_index, selected_nodes.loc[most_outstanding_outlier_df_index]

index, outlier = find_outlier_in_set(selected_nodes)
print(f"The most outstanding outlier is at index: {index}")
print(f"The outlier data:\n{outlier}")

X_test.iloc[index].equals(outlier)

"""# Experiment #1

"""

import numpy as np
import pandas as pd
from scipy import stats
np.random.seed(42)
def evaluate_outlier_detection(X_test, y_test, selected_nodes, num_samples=100, sample_size=20):
    outlier_results = []
    random_results = []

    for _ in range(num_samples):
        # Your method
        sample = selected_nodes.sample(n=sample_size)
        index, outlier = find_outlier_in_set(sample)
        outlier_results.append(1 if y_test.loc[index] == 1 else 0)

        # Random selection
        random_index = np.random.choice(X_test.index)
        random_results.append(1 if y_test.loc[random_index] == 1 else 0)

    outlier_percentage = np.mean(outlier_results) * 100
    random_percentage = np.mean(random_results) * 100

    # Perform statistical test
    _, p_value = stats.ttest_ind(outlier_results, random_results)

    return outlier_percentage, random_percentage, p_value

# Use the function
outlier_percentage, random_percentage, p_value = evaluate_outlier_detection(X_test, y_test, selected_nodes)

print(f"Percentage of employed outliers: {outlier_percentage:.2f}%")
print(f"Percentage of employed in random selection: {random_percentage:.2f}%")
print(f"P-value: {p_value:.4f}")

# Calculate improvement
improvement = (outlier_percentage - random_percentage) / random_percentage * 100
print(f"Improvement over random selection: {improvement:.2f}%")

import matplotlib.pyplot as plt
import numpy as np

def plot_comparison(outlier_percentage, random_percentage):
    # Values for the bar plot
    percentages = [random_percentage, outlier_percentage]
    labels = ['Random', 'Outlier']

    # Create the bar plot
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(labels, percentages, color=['lightblue', 'lightcoral'])

    # Customize the plot
    ax.set_xlabel('Comparison', fontsize=12)
    ax.set_ylabel('Percentage Employed', fontsize=12)
    ax.set_title('Percentage of Employed Individuals: Random vs. Outlier', fontsize=14)
    ax.set_ylim(0, 100)  # Set y-axis limit from 0 to 100%

    # Add value labels on top of each bar
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%',
                ha='center', va='bottom')

    # Add a horizontal line for the baseline (random selection)
    ax.axhline(y=random_percentage, color='gray', linestyle='--', alpha=0.7)
    ax.text(1.05, random_percentage, f'Baseline: {random_percentage:.1f}%',
            va='center', ha='left', color='gray')

    # Calculate and display improvement percentage
    improvement = (outlier_percentage - random_percentage) / random_percentage * 100
    ax.text(0.5, 0.95, f'Improvement: {improvement:.1f}%',
            ha='center', va='top', transform=ax.transAxes, fontsize=12,
            bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))

    # Display the plot
    plt.tight_layout()
    plt.show()



plot_comparison(outlier_percentage, random_percentage)

# Print additional statistics
print(f"Percentage of employed outliers: {outlier_percentage:.2f}%")
print(f"Percentage of employed in random selection: {random_percentage:.2f}%")
print(f"P-value: {p_value:.4f}")
print(f"Improvement over random selection: {(outlier_percentage - random_percentage) / random_percentage * 100:.2f}%")

"""#Exepriment #2"""

import numpy as np
import pandas as pd

# Number of random samples to test
num_samples = 10
sample_size = 100
experiment_results = []
outlier_vs_sample_results = []

def calculate_outlier_vs_sample(selected_nodes, random_seed):
    outlier_values = []
    sample_mean_values = []

    for _ in range(num_samples):
        # Randomly sample 100 rows from the dataset with a fixed random seed
        sample = selected_nodes.sample(n=sample_size, random_state=random_seed)

        # Find the most outstanding outlier in the sample
        index, outlier_value = find_outlier_in_set(sample)
        # print(outlier_value)

        outlier_values.append(outlier_value)

        # Calculate the mean of the sample
        sample_mean_value = sample.mean()
        # print(sample_mean_value)
        sample_mean_values.append(sample_mean_value)
    print(outlier_values)
    print(sample_mean_values)
    # Average results over all samples
    average_outlier_value = np.mean(outlier_values)
    average_sample_mean_value = np.mean(sample_mean_values)

    return average_outlier_value, average_sample_mean_value

# Assuming selected_nodes is your dataset (DataFrame)
random_seed = 43  # Set a fixed random seed
average_outlier_value, average_sample_mean_value = calculate_outlier_vs_sample(selected_nodes, random_seed)

print(f"The average value of the most outstanding outliers is: {average_outlier_value}")
print(f"The average mean value of the samples is: {average_sample_mean_value}")

labels = ['Sample Mean','Outlier']
values = [average_sample_mean_value.mean(), average_outlier_value.mean()]

plt.bar(labels, values, color=[ 'lightblue','lightcoral'])
plt.ylabel('Average Value')
plt.title('Comparison of Average Values: Outlier vs. Sample Mean')

plt.tight_layout()
plt.show()

"""#EXperiments #3


in this experiment first we are getting the index of the outlier in the selected.nodes/X_test. then we crandomly choose 0.1 of the selected nodes (the ones with higher probability to be a good match), then for each of these individuals we check their features and compare them with our best candidates. in each batch if our candidate is better than any of other person, it gets complete score (number of outlier being better than others/ total candidates in each batch).

at the end we get the average of all these scores

## the index of outlier in main dataframe
"""

X_test.iloc[index].equals(outlier)

"""## using random forest to get feature importance"""

rf =  RandomForestClassifier(n_estimators=100, max_depth=50, random_state=42)
rf.fit(X_train, y_train)

importances = rf.feature_importances_
feature_names = X_train.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Rank features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
print(feature_importance_df)

# Select top N features (example selecting top 10 features)
top_features = feature_importance_df['Feature'][:10].values
X_train_selected = X_train[top_features]
X_test_selected = X_test[top_features]

"""## outlier relation"""

X_test.iloc[index].equals(outlier)

selected_nodes.loc[index].equals(outlier)

"""#----------"""

weight_outlier = np.multiply((outlier).to_numpy(), importances)
print(weight_outlier.sum())
list_scores = []
list_weights = []
for i in range(20):
  sample_candidate = selected_nodes.sample(frac= 0.1)
  if index in sample_candidate.index:
    pass
  else:
    score = 0
    for ind in sample_candidate.index:
      sample = (selected_nodes.loc[ind]).to_numpy()

      weigtht = np.multiply(sample, importances)
      list_weights.append(weigtht.sum())
      if weigtht.sum() < weight_outlier.sum() :
        score += 1
    percent_score = score/len(sample_candidate)
    print(percent_score)
    list_scores.append(score/len(sample_candidate))


print(f"Average of how many times the outlier is better than every other candidates in each batch: {np.average(list_scores)}")

list_other_weights = []

for n in range(0,30):
  sample_candidate = X_test.sample(n = 20)
  list_scores = []
  list_weights2 = []
  for ind in sample_candidate.index:
    sample = (X_test.loc[ind]).to_numpy()

    weigtht = np.multiply(sample, importances)
    list_weights2.append(weigtht.sum())
  average_weight = np.mean(list_weights2)
  list_other_weights.append(average_weight)
list_other_weights = np.array(list_other_weights)

list_other_weights

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(5, 4))
y = np.arange(0, 1, 0.1)
ax.plot(np.arange(len(list_other_weights)), list_other_weights)

x_coord = [0, len(list_other_weights)]
y_coord = [weight_outlier.sum(), weight_outlier.sum()]

ax.set_xlim(1,30)
ax.set_ylim(0, 1)
ax.plot(x_coord, y_coord, linestyle='--', color='red', label='Outlier Sum')  # Customize as needed

plt.legend()
plt.show()

average_weight = np.mean(list_weights2)
average_weight , weight_outlier.sum()

"""#Experiment 4

choosing 50 people random, and between them choose 1 random person and one outlier, check which one is employed.
doing this for 100 times. to see how many of outlier persons are employed on average.
"""

sample_x = X_test.sample(n=100, random_state=None)

sample_x = X_test.sample(n=100, random_state=None)
probables = find_selected_nodes(sample_x)
index_out , out = find_outlier_in_set(probables)

# for candidate in sample_x:
#   its_weight = np.multiply((candidate).to_numpy(), importances).sum()

probables.loc[index_out].equals(out)
X_test.loc[index_out].equals(out)

X_test.loc[index_out].equals(out)

np.random.seed(42)
list_scores = []
list_of_employed = []
stats= 0
for i in range(0,10):
  score_weight = 0
  sample_x = X_test.sample(n=100, random_state=None)
  probables = find_selected_nodes(sample_x)
  index_out , out = find_outlier_in_set(probables)
  outlier_weight = np.multiply((out).to_numpy(), importances).sum()
  status = y_test.iloc[index]
  if status == 1:
    stats += 1
  for j in range(50):
    random_person = sample_x.sample(n=1, random_state=None)
    its_weight = np.multiply((random_person).to_numpy(), importances).sum()
    if random_person.index != index_out:
      if its_weight <= outlier_weight:
        score_weight += 1
  list_scores.append((score_weight/50)*100)
  print(f"for batch {i, j} the score of outlier is", score_weight/50 * 100)

print(np.mean(list_scores))
print(stats/10)

